{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Miembros del grupo:\n",
    "## Javier Cirugeda Bugallo\n",
    "## Hugo Franco Vargas Aponte\n",
    "## Salman Bouikou Nouinou"
   ],
   "id": "6d141bc6c8ff7b72"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-04T18:58:42.594197Z",
     "start_time": "2024-11-04T18:58:39.766656Z"
    }
   },
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "sc = SparkContext(\"local[*]\", \"Versión paralela\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/04 19:58:41 WARN Utils: Your hostname, ciruu-ms7b49 resolves to a loopback address: 127.0.1.1; using 192.168.0.32 instead (on interface enp0s31f6)\n",
      "24/11/04 19:58:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/04 19:58:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T18:59:11.391924Z",
     "start_time": "2024-11-04T18:59:11.359203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "rdd = sc.textFile(\"/home/ciruu/PycharmProjects/SPAI/test.csv\")"
   ],
   "id": "4d93e8e545e60d35",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T18:59:12.779057Z",
     "start_time": "2024-11-04T18:59:12.774203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def readRow(row):\n",
    "    arr = row.split(\",\")\n",
    "    lst = np.array(arr).astype(np.float64)\n",
    "    return np.array([float(x) for x in lst[:-1]]), int(lst[-1])\n",
    "\n",
    "def suma(el1, el2):\n",
    "    return el1 + el2"
   ],
   "id": "a418330a2acfbf3",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T18:59:15.661514Z",
     "start_time": "2024-11-04T18:59:14.189832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rdd1 = rdd.map(readRow)\n",
    "print(rdd1.take(2))\n",
    "\n",
    "num = rdd.count()\n",
    "print(num)\n",
    "\n",
    "# Dividimos entre el número total de filas\n",
    "def divide_by_num(tup):\n",
    "    tup[0] = tup[0] / num\n",
    "    return tup\n",
    "\n",
    "\n",
    "rdd2 = rdd1.reduce(lambda x, y: (x[0] + y[0], x[1]))\n",
    "\n",
    "print(rdd2)\n",
    "medias = rdd2[0] / num\n",
    "# Calculamos la media por cada columna con índice I\n",
    "#medias = rdd3.collect()\n",
    "print(f\"MEDIAS: {medias}\")\n",
    "#print(medias)"
   ],
   "id": "11160b826e456912",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([ 3.00388642e+00,  2.07699955e+03, -4.41037030e-05,  1.00000002e+01,\n",
      "        1.29114942e+04,  6.99998079e+01,  1.20000000e+01, -2.35688188e-08,\n",
      "        1.99000002e+02,  2.46836961e+09,  2.46836839e+09]), 1), (array([ 9.50811331e+02,  4.12285694e+04,  5.30000498e+01,  9.99999745e-01,\n",
      "        1.38579999e+04,  4.29592488e+06,  1.30000000e+01, -2.35688188e-08,\n",
      "        8.00000498e+00,  2.46836962e+09,  2.46837255e+09]), 1)]\n",
      "5000\n",
      "(array([6.15926485e+06, 1.03681121e+08, 3.63034537e+07, 3.00722501e+08,\n",
      "       6.24743675e+11, 7.62920618e+10, 4.61115064e+04, 9.28249311e+03,\n",
      "       6.20960335e+05, 1.06658485e+13, 1.13517042e+13]), 1)\n",
      "MEDIAS: [1.23185297e+03 2.07362242e+04 7.26069074e+03 6.01445003e+04\n",
      " 1.24948735e+08 1.52584124e+07 9.22230129e+00 1.85649862e+00\n",
      " 1.24192067e+02 2.13316969e+09 2.27034084e+09]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T18:59:17.032826Z",
     "start_time": "2024-11-04T18:59:17.029438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from math import sqrt\n",
    "# Función para obtener la media de un índice en concreto en un array\n",
    "def get_media_from_tup_array(arr, index):\n",
    "    for e in arr:\n",
    "        if e[0] == index:\n",
    "            return e[1]\n",
    "        \n",
    "        \n",
    "# Función para calcular la varianza a partir de una tupla\n",
    "def calc_var(tup):\n",
    "    #mean = get_media_from_tup_array(medias, tup[0])\n",
    "    return ((tup[0] - medias) ** 2), tup[1]"
   ],
   "id": "bce6b1e04e8db24a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T18:59:18.944689Z",
     "start_time": "2024-11-04T18:59:18.700698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Caćulo de la varianza a partir de las tuplas\n",
    "rdd4 = rdd1.map(calc_var)\n",
    "print(rdd4.take(2))\n",
    "rdd5 = rdd4.reduce(lambda x, y: (x[0] + y[0], x[1]))\n",
    "\n",
    "\n",
    "\n",
    "print(f\"VARIANZA: {rdd5[0]/num}\")"
   ],
   "id": "43a511465031e22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([1.51007007e+06, 3.48166664e+08, 5.27176306e+07, 3.61615812e+09,\n",
      "       1.56089600e+16, 2.32817011e+14, 7.71561013e+00, 3.44658722e+00,\n",
      "       5.59622713e+03, 1.12358987e+17, 3.92149115e+16]), 1), (array([7.89844029e+04, 4.19936211e+08, 5.19508051e+07, 3.61724063e+09,\n",
      "       1.56087235e+16, 1.20176132e+14, 1.42710073e+01, 3.44658722e+00,\n",
      "       1.35005953e+04, 1.12358990e+17, 3.92165577e+16]), 1)]\n",
      "VARIANZA: [2.51781240e+06 5.70220913e+08 2.81562405e+08 1.71636513e+10\n",
      " 5.62320496e+16 1.89501185e+15 2.73272537e+01 4.39041738e+00\n",
      " 8.19803933e+03 5.10634476e+17 1.67149262e+18]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T18:59:20.760398Z",
     "start_time": "2024-11-04T18:59:20.553580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#print(rdd5)\n",
    "rddx = rdd1.map(lambda x: (x[0])).stdev()\n",
    "print(rddx)\n",
    "print(f\"DESVIACIÓN ESTÁNDAR: {np.sqrt(rdd5[0]/num)}\")"
   ],
   "id": "500bd7aad330a6fc",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#print(rdd5)\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m rddx \u001B[38;5;241m=\u001B[39m \u001B[43mrdd1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstdev\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(rddx)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDESVIACIÓN ESTÁNDAR: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp\u001B[38;5;241m.\u001B[39msqrt(rdd5[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m/\u001B[39mnum)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/SPAI2/venv/lib/python3.12/site-packages/pyspark/rdd.py:2573\u001B[0m, in \u001B[0;36mRDD.stdev\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2550\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstdev\u001B[39m(\u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[NumberOrArray]\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[1;32m   2551\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2552\u001B[0m \u001B[38;5;124;03m    Compute the standard deviation of this RDD's elements.\u001B[39;00m\n\u001B[1;32m   2553\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2571\u001B[0m \u001B[38;5;124;03m    0.816...\u001B[39;00m\n\u001B[1;32m   2572\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 2573\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstats\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mstdev()\n",
      "File \u001B[0;32m~/PycharmProjects/SPAI2/venv/lib/python3.12/site-packages/pyspark/rdd.py:2343\u001B[0m, in \u001B[0;36mRDD.stats\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2340\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mredFunc\u001B[39m(left_counter: StatCounter, right_counter: StatCounter) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m StatCounter:\n\u001B[1;32m   2341\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m left_counter\u001B[38;5;241m.\u001B[39mmergeStats(right_counter)\n\u001B[0;32m-> 2343\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmapPartitions\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mStatCounter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   2344\u001B[0m \u001B[43m    \u001B[49m\u001B[43mredFunc\u001B[49m\n\u001B[1;32m   2345\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/SPAI2/venv/lib/python3.12/site-packages/pyspark/rdd.py:1926\u001B[0m, in \u001B[0;36mRDD.reduce\u001B[0;34m(self, f)\u001B[0m\n\u001B[1;32m   1924\u001B[0m vals \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmapPartitions(func)\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[1;32m   1925\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m vals:\n\u001B[0;32m-> 1926\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvals\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1927\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan not reduce() empty RDD\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/SPAI2/venv/lib/python3.12/site-packages/pyspark/util.py:83\u001B[0m, in \u001B[0;36mfail_on_stopiteration.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m     82\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 83\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     84\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m     85\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkRuntimeError(\n\u001B[1;32m     86\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSTOP_ITERATION_OCCURRED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     87\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{\n\u001B[1;32m     88\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexc\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(exc),\n\u001B[1;32m     89\u001B[0m             },\n\u001B[1;32m     90\u001B[0m         )\n",
      "File \u001B[0;32m~/PycharmProjects/SPAI2/venv/lib/python3.12/site-packages/pyspark/rdd.py:2341\u001B[0m, in \u001B[0;36mRDD.stats.<locals>.redFunc\u001B[0;34m(left_counter, right_counter)\u001B[0m\n\u001B[1;32m   2340\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mredFunc\u001B[39m(left_counter: StatCounter, right_counter: StatCounter) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m StatCounter:\n\u001B[0;32m-> 2341\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mleft_counter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmergeStats\u001B[49m\u001B[43m(\u001B[49m\u001B[43mright_counter\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/SPAI2/venv/lib/python3.12/site-packages/pyspark/statcounter.py:80\u001B[0m, in \u001B[0;36mStatCounter.mergeStats\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmu \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmu \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn \u001B[38;5;241m+\u001B[39m other\u001B[38;5;241m.\u001B[39mmu \u001B[38;5;241m*\u001B[39m other\u001B[38;5;241m.\u001B[39mn) \u001B[38;5;241m/\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn \u001B[38;5;241m+\u001B[39m other\u001B[38;5;241m.\u001B[39mn)\n\u001B[0;32m---> 80\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmaxValue \u001B[38;5;241m=\u001B[39m \u001B[43mmaximum\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmaxValue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmaxValue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mminValue \u001B[38;5;241m=\u001B[39m minimum(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mminValue, other\u001B[38;5;241m.\u001B[39mminValue)\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mm2 \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m other\u001B[38;5;241m.\u001B[39mm2 \u001B[38;5;241m+\u001B[39m (delta \u001B[38;5;241m*\u001B[39m delta \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn \u001B[38;5;241m*\u001B[39m other\u001B[38;5;241m.\u001B[39mn) \u001B[38;5;241m/\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn \u001B[38;5;241m+\u001B[39m other\u001B[38;5;241m.\u001B[39mn)\n",
      "\u001B[0;31mValueError\u001B[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PARTE 2 NORMALIZACIÓN",
   "id": "937e2e74e47dc195"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T18:59:22.466526Z",
     "start_time": "2024-11-04T18:59:22.462246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "array_medias = medias\n",
    "array_stdev = np.sqrt(rdd5[0]/num)\n",
    "print(array_stdev)\n",
    "print(array_medias)"
   ],
   "id": "6e0abab9c34d8e32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.58676161e+03 2.38792988e+04 1.67798214e+04 1.31010119e+05\n",
      " 2.37132979e+08 4.35317339e+07 5.22754758e+00 2.09533228e+00\n",
      " 9.05430247e+01 7.14586927e+08 1.29286218e+09]\n",
      "[1.23185297e+03 2.07362242e+04 7.26069074e+03 6.01445003e+04\n",
      " 1.24948735e+08 1.52584124e+07 9.22230129e+00 1.85649862e+00\n",
      " 1.24192067e+02 2.13316969e+09 2.27034084e+09]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T18:59:23.836942Z",
     "start_time": "2024-11-04T18:59:23.834114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize(tup):\n",
    "    data = tup[0]\n",
    "    normalized_data = []\n",
    "    i = 0\n",
    "    for e in data:\n",
    "        normalized_data.append((e - array_medias[i]) / array_stdev[i])  \n",
    "        i+=1\n",
    "    return np.array(normalized_data), tup[1]"
   ],
   "id": "5f30120e380c5dbe",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T19:00:11.069761Z",
     "start_time": "2024-11-04T19:00:10.878829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rdd2_1 = rdd1.map(normalize)\n",
    "med = rdd2_1.map(lambda x: x[0]).mean()\n",
    "print(med)\n",
    "#print(rdd2_1.collect())\n",
    "stdevv = rdd2_1.map(lambda x: x[0]).stdev()\n",
    "print(stdevv)\n",
    "#print(rdd1.collect())\n",
    "#print(rdd2_1.collect())"
   ],
   "id": "aa0c15c078b2fd95",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m rdd2_1 \u001B[38;5;241m=\u001B[39m rdd1\u001B[38;5;241m.\u001B[39mmap(normalize)\n\u001B[0;32m----> 2\u001B[0m med \u001B[38;5;241m=\u001B[39m \u001B[43mrdd2_1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(med)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#print(rdd2_1.collect())\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/SPAI2/venv/lib/python3.12/site-packages/pyspark/rdd.py:2523\u001B[0m, in \u001B[0;36mRDD.mean\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2501\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmean\u001B[39m(\u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[NumberOrArray]\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[1;32m   2502\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2503\u001B[0m \u001B[38;5;124;03m    Compute the mean of this RDD's elements.\u001B[39;00m\n\u001B[1;32m   2504\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2521\u001B[0m \u001B[38;5;124;03m    2.0\u001B[39;00m\n\u001B[1;32m   2522\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 2523\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstats\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mmean()\n",
      "File \u001B[0;32m~/PycharmProjects/SPAI2/venv/lib/python3.12/site-packages/pyspark/rdd.py:2343\u001B[0m, in \u001B[0;36mRDD.stats\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2340\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mredFunc\u001B[39m(left_counter: StatCounter, right_counter: StatCounter) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m StatCounter:\n\u001B[1;32m   2341\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m left_counter\u001B[38;5;241m.\u001B[39mmergeStats(right_counter)\n\u001B[0;32m-> 2343\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmapPartitions\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mStatCounter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   2344\u001B[0m \u001B[43m    \u001B[49m\u001B[43mredFunc\u001B[49m\n\u001B[1;32m   2345\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/SPAI2/venv/lib/python3.12/site-packages/pyspark/rdd.py:1926\u001B[0m, in \u001B[0;36mRDD.reduce\u001B[0;34m(self, f)\u001B[0m\n\u001B[1;32m   1924\u001B[0m vals \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmapPartitions(func)\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[1;32m   1925\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m vals:\n\u001B[0;32m-> 1926\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvals\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1927\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan not reduce() empty RDD\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/SPAI2/venv/lib/python3.12/site-packages/pyspark/util.py:83\u001B[0m, in \u001B[0;36mfail_on_stopiteration.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m     82\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 83\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     84\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m     85\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkRuntimeError(\n\u001B[1;32m     86\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSTOP_ITERATION_OCCURRED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     87\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{\n\u001B[1;32m     88\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexc\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(exc),\n\u001B[1;32m     89\u001B[0m             },\n\u001B[1;32m     90\u001B[0m         )\n",
      "File \u001B[0;32m~/PycharmProjects/SPAI2/venv/lib/python3.12/site-packages/pyspark/rdd.py:2341\u001B[0m, in \u001B[0;36mRDD.stats.<locals>.redFunc\u001B[0;34m(left_counter, right_counter)\u001B[0m\n\u001B[1;32m   2340\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mredFunc\u001B[39m(left_counter: StatCounter, right_counter: StatCounter) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m StatCounter:\n\u001B[0;32m-> 2341\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mleft_counter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmergeStats\u001B[49m\u001B[43m(\u001B[49m\u001B[43mright_counter\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/SPAI2/venv/lib/python3.12/site-packages/pyspark/statcounter.py:80\u001B[0m, in \u001B[0;36mStatCounter.mergeStats\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmu \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmu \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn \u001B[38;5;241m+\u001B[39m other\u001B[38;5;241m.\u001B[39mmu \u001B[38;5;241m*\u001B[39m other\u001B[38;5;241m.\u001B[39mn) \u001B[38;5;241m/\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn \u001B[38;5;241m+\u001B[39m other\u001B[38;5;241m.\u001B[39mn)\n\u001B[0;32m---> 80\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmaxValue \u001B[38;5;241m=\u001B[39m \u001B[43mmaximum\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmaxValue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmaxValue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mminValue \u001B[38;5;241m=\u001B[39m minimum(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mminValue, other\u001B[38;5;241m.\u001B[39mminValue)\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mm2 \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m other\u001B[38;5;241m.\u001B[39mm2 \u001B[38;5;241m+\u001B[39m (delta \u001B[38;5;241m*\u001B[39m delta \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn \u001B[38;5;241m*\u001B[39m other\u001B[38;5;241m.\u001B[39mn) \u001B[38;5;241m/\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn \u001B[38;5;241m+\u001B[39m other\u001B[38;5;241m.\u001B[39mn)\n",
      "\u001B[0;31mValueError\u001B[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Parte 3. Entrenamiento",
   "id": "13e97f05ff41817d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T18:59:49.265926Z",
     "start_time": "2024-11-04T18:59:49.253616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(RDD_Xy, iterations, learning_rate):\n",
    "    # Initialize weights (w) and bias (b)\n",
    "    n_features = 11  # As stated in the problem, there are 11 features\n",
    "    w = [0.0] * n_features  # Initialize weights to 0 for each feature\n",
    "    b = 0.0  # Initialize bias to 0\n",
    "\n",
    "    m = RDD_Xy.count()  # Get the number of training examples\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Compute the gradient for weights and bias\n",
    "        gradients = RDD_Xy.map(lambda xy: compute_gradients(xy, w, b)).reduce(lambda g1, g2: add_gradients(g1, g2))\n",
    "        \n",
    "        dw = [gradients[j] / m for j in range(n_features)]  # Average the weight gradients\n",
    "        db = gradients[-1] / m  # Average the bias gradient\n",
    "\n",
    "        # Update the weights and bias using gradient descent\n",
    "        w = [w[j] - learning_rate * dw[j] for j in range(n_features)]\n",
    "        b = b - learning_rate * db\n",
    "\n",
    "        # Calculate and print the cost for the current iteration\n",
    "        cost = compute_cost(RDD_Xy, w, b, m)\n",
    "        print(f\"iteration {i}  Cost: {cost}\")\n",
    "\n",
    "    return w, b\n",
    "\n",
    "def compute_gradients(xy, w, b):\n",
    "    \"\"\"\n",
    "    Compute gradients for a single example (X, y).\n",
    "    Returns:\n",
    "    A list containing [dw1, dw2, ..., dwk, db]\n",
    "    \"\"\"\n",
    "    X, y = xy\n",
    "    z = sum([w[j] * X[j] for j in range(len(X))]) + b  # Linear combination z = w.X + b\n",
    "    y_hat = 1 / (1 + math.exp(-z))  # Sigmoid function\n",
    "    \n",
    "    # Compute the gradients\n",
    "    dz = y_hat - y\n",
    "    dw = [dz * X[j] for j in range(len(X))]  # Derivative with respect to each weight\n",
    "    db = dz  # Derivative with respect to the bias\n",
    "\n",
    "    return dw + [db]\n",
    "\n",
    "def add_gradients(g1, g2):\n",
    "    \"\"\"\n",
    "    Add two gradient vectors element-wise.\n",
    "    \"\"\"\n",
    "    return [g1[j] + g2[j] for j in range(len(g1))]\n",
    "\n",
    "def compute_cost(RDD_Xy, w, b, m):\n",
    "    \"\"\"\n",
    "    Compute the cost (log loss) for the current weights w and bias b.\n",
    "\n",
    "    Arguments:\n",
    "    RDD_Xy -- RDD containing tuples (X, y) where:\n",
    "              X is the feature vector (list of floats)\n",
    "              y is the actual label (0 or 1)\n",
    "    w -- list of weights (length = number of features)\n",
    "    b -- bias (float)\n",
    "    m -- number of examples (integer)\n",
    "\n",
    "    Returns:\n",
    "    cost -- the value of the cost function (float)\n",
    "    \"\"\"\n",
    "    cost_sum = RDD_Xy.map(lambda xy: cost_per_example(xy, w, b)).sum()\n",
    "    cost = -cost_sum / m\n",
    "    return cost\n",
    "\n",
    "def cost_per_example(xy, w, b):\n",
    "    \"\"\"\n",
    "    Compute the cost for a single example (X, y).\n",
    "    \"\"\"\n",
    "    X, y = xy\n",
    "    z = sum([w[j] * X[j] for j in range(len(X))]) + b  # Linear combination z = w.X + b\n",
    "    y_hat = 1 / (1 + math.exp(-z))  # Sigmoid function\n",
    "\n",
    "    # Avoid log(0) by adding a small value (epsilon)\n",
    "    epsilon = 1e-12\n",
    "    cost = y * math.log(y_hat + epsilon) + (1 - y) * math.log(1 - y_hat + epsilon)\n",
    "    \n",
    "    return cost"
   ],
   "id": "8e68f1c686f9625e",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T18:59:55.679835Z",
     "start_time": "2024-11-04T18:59:52.246786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "iterations = 10\n",
    "learning_rate = 1.5\n",
    "w, b = train(rdd2_1, iterations, learning_rate)"
   ],
   "id": "6849f872d84f251b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0  Cost: 0.3948532040829545\n",
      "iteration 1  Cost: 0.3246878713366063\n",
      "iteration 2  Cost: 0.288619893109691\n",
      "iteration 3  Cost: 0.26577106221827723\n",
      "iteration 4  Cost: 0.24973102197549557\n",
      "iteration 5  Cost: 0.2377488614796198\n",
      "iteration 6  Cost: 0.22841332989502552\n",
      "iteration 7  Cost: 0.22091377189461728\n",
      "iteration 8  Cost: 0.21474653645562586\n",
      "iteration 9  Cost: 0.20958010243876835\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PARTE 4 PREDICCIÓN",
   "id": "8d45e7cead0403ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T18:59:58.030761Z",
     "start_time": "2024-11-04T18:59:58.021237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict(w, b, X):\n",
    "    z = sum([w[j] * X[j] for j in range(len(X))]) + b  # Linear combination z = w.X + b\n",
    "    y_hat = 1 / (1 + math.exp(-z))  # Sigmoid function\n",
    "    \n",
    "    # Convert y_hat to a binary label (0 or 1)\n",
    "    return 1 if y_hat >= 0.5 else 0\n",
    "\n",
    "def accuracy(w, b, RDD_Xy):\n",
    "    # Predict for each example in RDD_Xy and compare with actual label\n",
    "    correct_predictions = RDD_Xy.map(lambda xy: 1 if predict(w, b, xy[0]) == xy[1] else 0).sum()\n",
    "    \n",
    "    # Get the total number of examples in the RDD\n",
    "    total_examples = RDD_Xy.count()\n",
    "    \n",
    "    # Compute accuracy as the fraction of correct predictions\n",
    "    accuracy = correct_predictions / total_examples\n",
    "    \n",
    "    return accuracy"
   ],
   "id": "7f9067652629c5e",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T18:58:43.307932818Z",
     "start_time": "2024-10-19T08:19:47.827456Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "64833bff7517588d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PARTE 5 PRECISIÓN",
   "id": "e9d48ae6c9fb3181"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T19:00:01.913521Z",
     "start_time": "2024-11-04T19:00:01.637870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate accuracy\n",
    "acc = accuracy(w, b, rdd2_1)\n",
    "print(f\"Accuracy: {acc * 100:.4f}%\")"
   ],
   "id": "97498570ecb1dc12",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.9600%\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
